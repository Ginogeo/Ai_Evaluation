{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57441ef3-bff1-4d0a-9883-49ac06cc982e",
   "metadata": {},
   "source": [
    "## Installing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc1b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch torchvision diffusers transformers accelerate imageio imageio-ffmpeg ipywidgets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de0cf0-76a0-41ae-ad5e-9c24c4851f13",
   "metadata": {},
   "source": [
    "## Importing dependencies and setting up cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba40a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "Model cache directory: /teamspace/studios/this_studio/models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Set custom cache directory for model downloads\n",
    "\n",
    "cache_dir = \"/teamspace/studios/this_studio/models\"\n",
    "pathlib.Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "# Set environment variables BEFORE importing huggingface libraries\n",
    "os.environ[\"HF_HOME\"] = cache_dir\n",
    "# Create hub subdirectory\n",
    "pathlib.Path(os.path.join(cache_dir, \"hub\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Video\n",
    "from diffusers import WanPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Model cache directory: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c6d9b-f8d8-4b48-aad6-8ac71cb0d2b5",
   "metadata": {},
   "source": [
    "## Mapping style and camera prompts , specify model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070df967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"  \n",
    "OUTPUT_DIR = \"generated_videos\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "STYLE_PROMPTS = {\n",
    "    \"cinematic\": \"cinematic lighting, film grain, dramatic shadows, professional cinematography\",\n",
    "    \"cartoon\": \"cartoon style, animated, vibrant colors, hand-drawn aesthetic\",\n",
    "    \"realistic\": \"photorealistic, 4K quality, natural lighting, detailed textures\",\n",
    "    \"anime\": \"anime style, Japanese animation, cel shading, expressive\",\n",
    "    \"vintage\": \"vintage film, retro aesthetic, faded colors, nostalgic\"\n",
    "}\n",
    "CAMERA_PROMPTS = {\n",
    "    \"front view\": \"front view, facing camera\",\n",
    "    \"side view\": \"side view, profile shot\",\n",
    "    \"top-down\": \"top-down view, bird's eye perspective, overhead shot\",\n",
    "    \"low angle\": \"low angle shot, looking up, dramatic perspective\",\n",
    "    \"close-up\": \"close-up shot, detailed focus\",\n",
    "    \"wide shot\": \"wide shot, establishing shot, full scene view\"\n",
    "}\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b0392-0e13-4e2a-8117-5c4518760f8e",
   "metadata": {},
   "source": [
    "## Downloading the models , setting precision(bf16) and enables cpu offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87335a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = None\n",
    "def load_pipeline():\n",
    "    \"\"\"Load the Wan 2.1 T2V 1.3B pipeline.\"\"\"\n",
    "    global pipe\n",
    "    if pipe is not None:\n",
    "        return pipe\n",
    "    # Load the full pipeline directly (includes VAE, transformer, scheduler, etc.)\n",
    "    pipe = WanPipeline.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    # Enable memory optimizations\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return pipe\n",
    "    \n",
    "print(\"Downloading and loading model... This may take a while on first run.\")\n",
    "pipe = load_pipeline()\n",
    "print(\"Model ready for video generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6b8b3-bc5e-4d8c-b0f2-976100faf46f",
   "metadata": {},
   "source": [
    "**Three functions that**\n",
    " 1. **convert style and camera selection to prompts** \n",
    " 2. **convert secconds to no of frames**\n",
    " 3. **generate video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9675cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(subject_action: str, style: str, camera_angle: str) -> str:\n",
    "    \"\"\"Construct an enhanced prompt from user inputs.\"\"\"\n",
    "    style_desc = STYLE_PROMPTS.get(style, style)\n",
    "    camera_desc = CAMERA_PROMPTS.get(camera_angle, camera_angle)\n",
    "    full_prompt = f\"{subject_action}, {camera_desc}, {style_desc}, high quality, smooth motion\"\n",
    "    return full_prompt\n",
    "\n",
    "def duration_to_frames(duration_seconds: float, fps: int = 16) -> int:\n",
    "    \"\"\"Convert duration in seconds to number of frames.\"\"\"\n",
    "    target_frames = int(duration_seconds * fps)\n",
    "    valid_frames = [17, 25, 33, 41, 49, 57, 65, 81, 97, 113, 129, 145, 161]\n",
    "    return min(valid_frames, key=lambda x: abs(x - target_frames))\n",
    "\n",
    "def generate_video(subject_action: str, style: str, camera_angle: str, \n",
    "                   duration: float, seed: int = -1) -> tuple:\n",
    "    \"\"\"Generate video from text prompt.\"\"\"\n",
    "    \n",
    "    pipe = load_pipeline()\n",
    "    \n",
    "    # Construct prompt\n",
    "    prompt = construct_prompt(subject_action, style, camera_angle)\n",
    "    print(f\"Generated prompt: {prompt}\")\n",
    "    \n",
    "    # Calculate frames\n",
    "    num_frames = duration_to_frames(duration)\n",
    "    print(f\"Generating {num_frames} frames (~{num_frames/16:.1f}s at 16fps)...\")\n",
    "    \n",
    "    # Set seed\n",
    "    if seed == -1:\n",
    "        seed = torch.randint(0, 2**32, (1,)).item()\n",
    "    generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "    print(f\"Using seed: {seed}\")\n",
    "    \n",
    "    negative_prompt = \"blurry, low quality, distorted, deformed, static, no motion\"\n",
    "    \n",
    "    # Generate video\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=480, width=832,\n",
    "        num_frames=num_frames,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=5.0,\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # Save video\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"video_{timestamp}.mp4\")\n",
    "    export_to_video(output.frames[0], output_path, fps=16)\n",
    "    \n",
    "    # Cleanup\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Video saved to: {output_path}\")\n",
    "    return output_path, prompt, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5c6ac-a7e4-4ec2-9cff-f4359f7d6032",
   "metadata": {},
   "source": [
    "## Interactive video generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2126f1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c007337fbd6749c585f8ebb2b64bc421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ðŸŽ¬ Text-to-Video Generator</h3>'), Textarea(value='A cat playing with a ball in â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Interactive UI Widgets\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"A cat playing with a ball in a sunny garden\",\n",
    "    placeholder=\"Enter subject + action (e.g., 'A dog running in a park')\",\n",
    "    description=\"Prompt:\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"80px\")\n",
    ")\n",
    "\n",
    "style_dropdown = widgets.Dropdown(\n",
    "    options=[\"cinematic\", \"cartoon\", \"realistic\", \"anime\", \"vintage\"],\n",
    "    value=\"cinematic\",\n",
    "    description=\"Style:\"\n",
    ")\n",
    "\n",
    "camera_dropdown = widgets.Dropdown(\n",
    "    options=[\"front view\", \"side view\", \"top-down\", \"low angle\", \"close-up\", \"wide shot\"],\n",
    "    value=\"front view\",\n",
    "    description=\"Camera:\"\n",
    ")\n",
    "\n",
    "duration_slider = widgets.FloatSlider(\n",
    "    value=2.0, min=1.0, max=10.0, step=0.5,\n",
    "    description=\"Duration (s):\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "seed_input = widgets.IntText(value=-1, description=\"Seed:\", tooltip=\"-1 for random seed\")\n",
    "\n",
    "generate_btn = widgets.Button(description=\"Generate Video\", button_style=\"success\", icon=\"play\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_generate_click(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(\"Starting Text-to-Video generation...\")\n",
    "        try:\n",
    "            video_path, final_prompt, used_seed = generate_video(\n",
    "                prompt_input.value,\n",
    "                style_dropdown.value,\n",
    "                camera_dropdown.value,\n",
    "                duration_slider.value,\n",
    "                seed_input.value\n",
    "            )\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"Generation Complete!\")\n",
    "            print(f\"Seed used: {used_seed}\")\n",
    "            display(Video(video_path, embed=True, width=640))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "generate_btn.on_click(on_generate_click)\n",
    "\n",
    "# Display UI\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>ðŸŽ¬ Text-to-Video Generator</h3>\"),\n",
    "    prompt_input,\n",
    "    widgets.HBox([style_dropdown, camera_dropdown]),\n",
    "    widgets.HBox([duration_slider, seed_input]),\n",
    "    generate_btn,\n",
    "    output_area\n",
    "])\n",
    "display(ui)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
